SUMMARY OF DBDA  PROJECT STEPS: 						
(last updated - 11/01/2022)											    

																						
#-steps still not performed or are to be performed
without #-Steps performed.EXTRACT PHASE:
commands are enclosed within ---------'     '-----------------
files and directory names are at end of ------------------"     "
* - planned to do, need confirmation

EXTRACT:

1. Gathered data of opening and closing prices of 1550 companies (NSE) using jugaadtrader python program for zerodha api and automated the process using BASH script. ---- DataGatherStock
2. Gathered additional parameter of market performance of each of these 1550 companies, here 7 companies data gathering failed. So total comapanies we are considering for Analysis are now 1543 companies data.
3. These additional parameters were collected from screener.in using selenium in python(created from scratch) for doing browser automation.  -----screenerdata2
	Command used to excute the data gathering from BASH shell ------- date > logs.txt && python main.py >> logs.txt ------------
4. The filenames of .csv from step 1 and .xlsx from step 2 were Symbol and company names respectively.
5. So the file names from step 2 needed to be changed to the Symbols.
6. To do so, we used the script which was used to gather the data in step 2, deleted the companies that failed to gather from this list.
7. Now we listed the filenames in data1 folder of step 3 according to the firstmodified basis usinfg the command ---ls -ltr -----
8. then cut down the other columns from result using -----ls -ltr | cut --complement -d " " -f 1-8 ------
9. Now cut down the file extension and transfer the result to a txt file ------ls -ltr | cut --complement -d " " -f 1-8 | sed 's/.xlsx//g' > filenames.txt------------
10. Now created a csv file containing the Symbol names & the file names gathered. ------------Symbolnamemap.csv
#11. Now created a python program to rename the files accordingly to Symbol names.
#12. Created a pandas notebook to extract the required data from xlsx files from step 3 and create a csv file of them.
#Now we have the csv files of both data from step 1 and step 2.
#13. Look into data and clean the data to attain required data quality.
*#13. write a python program to put the both csv files of companies in a single directory naming them the symbol of company names.

LOAD PHASE:

#14. Ingesting the data into the hdfs using spark and meanwhile also converting them to parquet format.
#15.  Studying the queries used to analysis of data and accordingly figuring the file distribution on hdfs.

TRANSFORM PHASE:

#16. Applying the joins, views on the data and creating a structure ready to be queried.
